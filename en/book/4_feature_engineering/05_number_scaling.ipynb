{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4972aa",
   "metadata": {},
   "source": [
    "# Escalamiento de números"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233508f3",
   "metadata": {},
   "source": [
    "When working with data, it's common to encounter variables with different scales or magnitudes. For example, imagine a medical dataset where we can find information related to people's weight and height. In this dataset, weight varies between 50 and 150 kilograms, while height varies between 1.50 and 1.90 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbd703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataset(n):\n",
    "    min_w, max_w = 50, 150\n",
    "    noise_w = np.random.normal(0, 10, n)\n",
    "    weights = np.random.uniform(min_w, max_w, n)\n",
    "    \n",
    "    min_h, max_h = 1.50, 1.90\n",
    "    noise_h = np.random.normal(0, 10, n)\n",
    "    heights = np.random.uniform(min_h, max_h, n)\n",
    "    return np.vstack([weights, heights]).T\n",
    "    \n",
    "def plot(ax, dataset, title):\n",
    "    weights, heights = dataset[:,0], dataset[:,1]\n",
    "    noise = np.random.uniform(-0.2, 0.2, len(weights))\n",
    "    ax.scatter(\n",
    "        weights,\n",
    "        np.full(len(weights), 1) + noise,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        heights,\n",
    "        np.full(len(heights), 2) + noise,\n",
    "    )\n",
    "    ax.set_ylim(0.5, 2.5)\n",
    "    ax.set_yticks([1, 2], ['Weight','Height'])\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "def show_dataframe(dataset):\n",
    "    return pd.DataFrame(dataset, columns=['Weight', 'Height'])\n",
    "\n",
    "def plot_dataset(*objects):\n",
    "    objects = [(objects[i], objects[i+1]) for i in range(0, len(objects) -1, 2)]\n",
    "    plots = len(objects)\n",
    "    fig, axs = plt.subplots(1, plots, figsize=(5 * plots, 5))\n",
    "    if len(objects) == 1:\n",
    "        axs = [axs]\n",
    "    for (dataset, title), ax in zip(objects, axs):\n",
    "        plot(ax, dataset, title)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3dc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = make_dataset(100)\n",
    "show_dataframe(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "plot(ax, original_dataset, \"Original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6764ef",
   "metadata": {},
   "source": [
    "$$\n",
    "  X_{norm} = (X - μ) / σ\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57827726",
   "metadata": {},
   "source": [
    "Our machine learning model has no notion that some things are measured in kilograms and others in meters, and if the data is not scaled, some attributes may have more weight than others due to their scales, which can lead to incorrect decisions by the model.\n",
    "\n",
    "Returning to our example, the algorithm may \"focus\" more on weight since it has a larger variance range, 100, while weight only has 0.4. This is where the importance of scaling our variables comes in.\n",
    "\n",
    "## Why is it important?\n",
    "\n",
    "Specifically, we can think of three reasons why it's worth scaling the values:\n",
    "\n",
    " 1. It facilitates training: By having all features on the same scale, Machine Learning algorithms converge faster towards a minimum in the loss function.\n",
    " 1. It improves performance: Some algorithms, like SVM or KNN, which are based on distances, are very sensitive to the scale of the data and can give incorrect results if the features are not standardized.\n",
    " 1. It allows for easier interpretation: By standardizing, we can compare the relative importance of features in our model.\n",
    "\n",
    "## How to do it?\n",
    "\n",
    "There are various techniques to achieve variable scaling, and we can use the most common ones with scikit-learn.\n",
    "\n",
    "### Standardization\n",
    "\n",
    "Standardization is perhaps the most common of scaling transformations. It consists of centering all the data of a given attribute in the set at 0 and making its variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cf1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "standard_scaled = scaler.fit_transform(original_dataset)\n",
    "\n",
    "plot_dataset(\n",
    "    original_dataset, \"Original data\",\n",
    "    standard_scaled, \"Standardized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a599c7",
   "metadata": {},
   "source": [
    "$$\n",
    "  X_{norm} = (X - X_{min}) / (X_{max} - X_{min})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4420e",
   "metadata": {},
   "source": [
    "This scaler is commonly used when you have normally distributed data and want all your data to have similar scales. Also note that the range of features is variable. It is also used when preparing data for regression or neural networks.\n",
    "\n",
    "### Min-max scaling\n",
    "\n",
    "This scaling technique helps us transform the values of our dataset so that they fall within a known range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "minmax_dataset = scaler.fit_transform(original_dataset)\n",
    "\n",
    "plot_dataset(\n",
    "    original_dataset, \"Original data\",\n",
    "    minmax_dataset, \"Min-max scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c246486",
   "metadata": {},
   "source": [
    "$$\n",
    "  X_{scaled} = X / max(abs(X))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941b94f",
   "metadata": {},
   "source": [
    "Use this scaler when you want the data to be within a specific range, in this case, between 0 and 1 by default - especially useful for distance-based models, such as the k-Nearest neighbors model or SVM. In this case, it doesn't matter much if your features are normally distributed.\n",
    "\n",
    "### Maximum absolute scaling\n",
    "\n",
    "This scaler transforms the data by dividing it by the maximum absolute value of each variable. This is useful when working with data that has very large or very small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "maxabs_scaled = scaler.fit_transform(original_dataset)\n",
    "\n",
    "plot_dataset(\n",
    "    original_dataset, \"Original data\",\n",
    "    maxabs_scaled, \"Max-abs scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9cfad",
   "metadata": {},
   "source": [
    "`MaxAbsScaler` is a good choice when features are sparse or mostly zero and have variable scales. It's also useful when using neural networks or sparse linear models such as Logistic Regression or SVM.\n",
    "\n",
    "### Other ways to scale values\n",
    "\n",
    "Scikit-learn also has other scalers that we won't be able to cover here, but which are more specialized for working with data with other distributions that help transform data into scaled values with normal distributions for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d5728",
   "metadata": {},
   "source": [
    "There you have it, I hope you understood the value of scaling your data and that from now on you'll use this technique in your projects.\n",
    "\n",
    "In the next chapter, we'll learn how we can convert continuous variables to discrete ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
