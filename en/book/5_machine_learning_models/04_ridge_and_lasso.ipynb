{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07104c",
   "metadata": {},
   "source": [
    "# Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837e88",
   "metadata": {},
   "source": [
    "Hello, let's continue talking about regression.\n",
    "\n",
    "There are two other types of regressions in scikit-learn: Lasso and Ridge. These are characterized by placing restrictions on the magnitude of the regression coefficients.\n",
    "\n",
    "## Ridge\n",
    "\n",
    "The first one I want to talk about is known as Ridge.\n",
    "\n",
    "This regression, unlike traditional linear regression, penalizes the magnitude of the learned coefficients. This in turn reduces the variance of the estimated coefficients and improves the stability of the model when collinearity exists, that is, the correlation between our predictor variables.\n",
    "\n",
    "The penalty used by Ridge is known as L2; I've included more details about this penalty in the book's resources for you to learn more.\n",
    "\n",
    "Before we begin, let's generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, bias=2.0, noise=5.0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9eeb95",
   "metadata": {},
   "source": [
    "To use ridge, you need to import it from `linear_model`, and call the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0df22",
   "metadata": {},
   "source": [
    "And like every other estimator in scikit-learn, it has the `fit` and `predict` methods to interact with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92539c73",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "The class shares a couple of arguments with `LinearRegression`, these are `fit_intercept` and `normalize`. But it also includes some specific ones:\n",
    "\n",
    " - `alpha`: This is a float parameter that specifies the level of regularization in the model. A higher alpha value results in smaller coefficients and, therefore, a more simplified model. The default value is 1.0 – this is a hyperparameter that is recommended to tune.\n",
    " - `solver`: This is a string that indicates the solver used in the underlying optimization problem. Possible values are \"auto\", \"svd\", \"cholesky\", \"lsqr\", and \"sparse_cg\". The default value is \"auto\", and it generally works well.\n",
    " - `max_iter`: This is an integer that specifies the maximum number of iterations allowed in the solver – some solvers work iteratively. The default value is None, which means a reasonable value is used based on the size of the dataset.\n",
    "\n",
    "## Lasso\n",
    "\n",
    "This regression, unlike traditional linear regression, penalizes the magnitude of learned coefficients – similar to Ridge regression.\n",
    "\n",
    "The penalty used by Lasso is known as L1; I leave more details about this penalty in the book's resources for you to learn more. But something to note is that Lasso can force some coefficients to become zero, thus excluding some of the input variables from the model's calculations, thereby reducing its complexity.\n",
    "\n",
    "The \"punished\" variables are those that the model considers irrelevant or with high collinearity.\n",
    "\n",
    "The Lasso algorithm works iteratively by definition, which is another difference from traditional linear regression that has a closed analytical solution.\n",
    "\n",
    "Ridge is also available in the `linear_model` module of `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3cefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98af71b",
   "metadata": {},
   "source": [
    "And of course, it shares the interface of other estimators in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c99446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4197b476",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "Like Ridge regression, it also has the `alpha` argument to control the strength with which it applies the penalty, as well as the `max_iter` parameter which is more important here because this is a fully iterative algorithm.\n",
    "\n",
    "It also has the following arguments that can help you in training:\n",
    "\n",
    " - `tol`: This is the tolerance for the convergence of the optimization algorithm. If the difference between two consecutive iterations is less than `tol`, the algorithm is considered to have converged. The default value is 1e-4.\n",
    " - `warm_start`: This parameter is boolean and specifies whether to use the coefficients from the previous regression as a starting point for the current regression. If `True`, the previous solution is used as a starting point for optimization, which can speed up the fitting process. The default value is `False`.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "Both classes offer the linear regression attributes that help us understand a bit more about our input values. If you remember, in the previous chapter on linear regression we saw how the `coef_` and `intercept_` attributes can be used to interpret the results. Lasso and Ridge have them too.\n",
    "\n",
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63371c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import  make_regression\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "# Generate a random regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, noise=10, random_state=42)\n",
    "\n",
    "# Fit Linear Regression\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "# Fit regressions\n",
    "lr.fit(X, y)\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Plot the coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "coefficients = [lr.coef_, ridge.coef_, lasso.coef_]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (model, coef) in enumerate(zip(models, coefficients)):\n",
    "    ax.bar(np.arange(len(coef)) + i*0.25, coef, color=colors[i], width=0.25, label=model)\n",
    "\n",
    "ax.set_xticks(np.arange(len(coef)))\n",
    "ax.set_xticklabels(['Feature '+str(i) for i in range(len(coef))])\n",
    "ax.set_ylabel(\"Coeficiente\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d692f4a",
   "metadata": {},
   "source": [
    "## When to use each one?\n",
    "\n",
    " - Linear regression is a good option when the relationship between the independent variables and the dependent variable is approximately linear. It's always a good method to consider, even if just to establish a baseline.\n",
    " - Ridge regression is a good choice when there are many features and some of them are expected to have small or moderate effects on the dependent variable. Ridge helps with regularization by shrinking some feature coefficients towards zero, but not to zero like Lasso.\n",
    " - Lasso regression is a good option when there are many features and some of them are expected to be irrelevant or redundant. Lasso helps with feature selection and regularization by turning some feature coefficients to zero, effectively removing them from the model.\n",
    "And there you have it, two other types of regression very similar to linear regression but including a level of penalization for the coefficients that helps us reduce overfitting and the overall complexity of the model.\n",
    "\n",
    "The next chapter will continue exploring supervised learning."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
