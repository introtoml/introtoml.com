{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8112dd03",
   "metadata": {},
   "source": [
    "# k-Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e56cd",
   "metadata": {},
   "source": [
    "Hello again, welcome to the machine learning book with Scikit-learn. This is the last lesson on classification models.\n",
    "\n",
    "Another way to classify elements is through the k-Nearest Neighbors algorithm. This works by classifying data based on the labels of the data closest to it in the feature space – for each new sample, the \"k\" nearest neighbors are sought, and depending on these labels, it is decided which class a new element belongs to.\n",
    "\n",
    "The way to use it in scikit-learn is as simple as any other classifier, we import it from the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d93070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c9677",
   "metadata": {},
   "source": [
    "And it has the usual `fit` and `predict` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, random_state=42, noise=0.1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(knn.predict(X_test))\n",
    "print(knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53ec9c",
   "metadata": {},
   "source": [
    "It also has the `predict_proba` method, although the probability here helps us define how many close neighbors it had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce67b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.predict_proba(X_test)[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14223d",
   "metadata": {},
   "source": [
    "Anyway, the interesting part lies in the arguments, the hyperparameters of the class.\n",
    "\n",
    "## Arguments\n",
    "\n",
    "Like many other machine learning models, the `KNeighborsClassifier` class has some arguments to modify its behavior:\n",
    "\n",
    " - `n_neighbors`: This hyperparameter determines the number of neighbors to be used in the classification. If the value of `n_neighbors` is too low, the model may overfit the data, while if the value is too high, the model may underfit the data. The default value is 5.\n",
    " - `weights`: This hyperparameter determines how the distances between the training samples and the test sample are weighted. The options are 'uniform', where all samples have the same weight in the classification, and 'distance', where closer samples have a greater weight. Usually, the default option is 'uniform'.\n",
    " - `metric`: This hyperparameter determines the distance metric used to calculate the distances between samples. Some common options are 'euclidean', 'manhattan', and 'minkowski'.\n",
    " - `algorithm`: This hyperparameter determines the algorithm used to find the nearest neighbors. The options are 'brute', which searches for the nearest neighbors by calculating all distances between all samples, and 'kd_tree' or 'ball_tree', which use data structures to search for the nearest neighbors more efficiently.\n",
    "Let's see the behavior of the model when we modify its, starting with what may be the most important one:\n",
    "\n",
    "First, let's create an example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f89c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_boundaries\n",
    "\n",
    "X, y = make_moons(n_samples=1000, random_state=42, noise=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838a223",
   "metadata": {},
   "source": [
    "## `n_neighbors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(\n",
    "    X, y, \n",
    "    [\n",
    "        ('n_neighbors = 1', KNeighborsClassifier(n_neighbors=1)),\n",
    "        ('n_neighbors = 10', KNeighborsClassifier(n_neighbors=10)),\n",
    "        ('n_neighbors = 100', KNeighborsClassifier(n_neighbors=100)),\n",
    "        ('n_neighbors = 999', KNeighborsClassifier(n_neighbors=999)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d207c51",
   "metadata": {},
   "source": [
    "## The importance of scaling features\n",
    "\n",
    "k-Nearest Neighbors is an algorithm entirely based on distances between features, so it is vitally important that these are scaled before passing them to the model. Otherwise, you will experience problems when training and obtaining predictions.\n",
    "\n",
    "To demonstrate this, here I am creating a dataset and taking one of its features out of scale by multiplying it by 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, random_state=42, noise=.1)\n",
    "X[:,1] = X[:,1] * 5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed4bd1",
   "metadata": {},
   "source": [
    "Then I train a dataset with unscaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "accuracy_unscaled = knn_unscaled.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a7f59",
   "metadata": {},
   "source": [
    "And I train one by scaling the features beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f156e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "accuracy_scaled = knn_scaled.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8bba7",
   "metadata": {},
   "source": [
    "From the outset, we can see the difference in performance between one and the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20caabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sin escalar\\t{accuracy_unscaled:.4f}\")\n",
    "print(f\"Escaladas\\t{accuracy_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a27e7",
   "metadata": {},
   "source": [
    "But it can be better appreciated with a two-dimensional graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9664d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_knn_boundaries\n",
    "\n",
    "plot_knn_boundaries(knn_unscaled,knn_scaled, X_train, X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f169aca",
   "metadata": {},
   "source": [
    "## Size\n",
    "\n",
    "The size of a kNN model on disk and in memory varies with respect to the size of its training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "n_samples = [100, 1000, 10000, 100000]\n",
    "\n",
    "for n in n_samples:\n",
    "    X, y = make_classification(n_samples=n, n_features=20)\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X, y)\n",
    "    joblib.dump(knn, f\"/tmp/knn_model_{n}.joblib\")\n",
    "    model_size = os.path.getsize(f\"/tmp/knn_model_{n}.joblib\")\n",
    "\n",
    "    print(f\"Tamaño del modelo (n={n}):\\t{model_size:>10} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42a3b",
   "metadata": {},
   "source": [
    "## In conclusion\n",
    "\n",
    "The k-NN model is one that you can use in classification problems, especially in small or moderate-sized datasets, problems with multiple classes, noisy data or missing values, and in problems with low to moderate dimensionality.\n",
    "\n",
    "But consider not using it in problems with large datasets, problems with high dimensionality, problems where speed is critical, problems with very sparse data, and in problems where accuracy is more important than simplicity and interpretability. In these cases, it may be necessary to consider other machine learning algorithms more suitable for the specific problem.\n",
    "\n",
    "We'll see you in the next chapter, where we'll discuss another distance-based model."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
