{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a820eebe",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d6e0b",
   "metadata": {},
   "source": [
    "Continuing with clustering algorithms, there is another one that, unlike k-Means, does not require you to specify the number of clusters in advance.\n",
    "\n",
    "This algorithm is known as DBSCAN, or *Density-Based Spatial Clustering of Applications with Noise*.\n",
    "\n",
    "This algorithm groups elements within a set based on their density in space. Points that are close to each other will be considered part of the same cluster, while points that are very far apart will be considered as noise.\n",
    "\n",
    "The implementation of `DBSCAN` is within the `sklearn.cluster` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e47d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef04be",
   "metadata": {},
   "source": [
    "Let's generate a dataset with 5 data blobs, originally 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf960818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=500, centers=5, cluster_std=0.8, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859075b",
   "metadata": {},
   "source": [
    "As with `DBSCAN`, it does not require specifying the number of clusters, so we can initialize it with its default values â€“ and then we'll use `fit_predict` to obtain the assigned clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d23ce",
   "metadata": {},
   "source": [
    "If we review the labels, you'll see that some have the value `-1`, these are the ones that were identified as noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b55fd",
   "metadata": {},
   "source": [
    "We can visualize the clusters with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import view_dbscan\n",
    "\n",
    "view_dbscan(X, y_true, [(\"Etiquetas predichas\", dbscan)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683ec4b",
   "metadata": {},
   "source": [
    "## Arguments of `DBSCAN`\n",
    "\n",
    "DBSCAN has several arguments, but the most important ones to consider are:\n",
    "\n",
    " - `eps`: The neighborhood radius that defines the maximum distance between two points to be considered neighbors.\n",
    " - `min_samples`: The minimum number of points required to form a cluster. Values of `min_samples` that are too small can result in very small clusters and unwanted noise, while values that are too large can cause fewer points to be grouped.\n",
    "\n",
    "## Visualization of the arguments\n",
    "\n",
    "### `eps`\n",
    "\n",
    "The neighborhood radius that defines the maximum distance between two points to be considered neighbors. Values of `eps` that are too small can cause fewer points to be grouped or even all points to be classified as noise, while values that are too large can group points that shouldn't be together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "eps_list = [0.01, 0.1, 0.3, 0.5, 1]\n",
    "\n",
    "trained_dbscans = []\n",
    "for eps_value in eps_list:\n",
    "    dbscan = DBSCAN(eps = eps_value)\n",
    "    dbscan.fit(X)\n",
    "    trained_dbscans.append((f\"eps = {eps_value}\", dbscan))\n",
    "\n",
    "view_dbscan(X, y_true, trained_dbscans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343f02f",
   "metadata": {},
   "source": [
    "### `min_samples`\n",
    "\n",
    "The minimum number of points required to form a cluster. `min_samples` values that are too small can result in very small clusters and unwanted noise, while values that are too large can cause fewer points to be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "min_samples_list = [1, 3, 5, 20, 50]\n",
    "\n",
    "trained_dbscans = []\n",
    "for min_samples_value in min_samples_list:\n",
    "    dbscan = DBSCAN(min_samples = min_samples_value)\n",
    "    dbscan.fit(X)\n",
    "    trained_dbscans.append((f\"min_samples = {min_samples_value}\", dbscan))\n",
    "\n",
    "view_dbscan(X, y_true, trained_dbscans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca0b77",
   "metadata": {},
   "source": [
    "## To choose hyperparameters\n",
    "\n",
    "To measure the quality of our hyperparameter selection in dbscan, we can use the metrics we previously discussed such as the Silhouette coefficient, the Calinski-Harabasz index, or the Davies-Bouldin index to find the best hyperparameter configuration.\n",
    "\n",
    "You can also use secondary, business-related metrics to define the best values.\n",
    "\n",
    "## Compared to k-Means\n",
    "\n",
    "DBSCAN is more suitable than K-means in situations where the number of clusters is unknown, clusters have non-convex shapes or different densities, and the data contains noise or outliers. In general, DBSCAN is a good choice when a more automated solution is desired, one that is less sensitive to ad-hoc assumptions about the number of clusters and the shape of the data.\n",
    "\n",
    "Now you know two clustering algorithms available in Scikit Learn. It's time to explore other unsupervised learning techniques."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
