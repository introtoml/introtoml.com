{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e722d7ee",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa70fe5",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a reduction technique used to prepare data for visualization or for use in analysis and modeling.\n",
    "\n",
    "PCA aims to reduce the dimensionality of the data by projecting it onto a new feature space composed of the directions of maximum variance in the original data.\n",
    "\n",
    "The goal of PCA is to find a new representation of the data that preserves as much information as possible while reducing the number of variables. This is achieved by identifying the principal components, which are linear combinations of the original variables that explain the largest amount of variance in the data.\n",
    "\n",
    "We will work with the Iris flowers dataset, which has 4 dimensions, each of which has a relationship with the real world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92558620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "X[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e52d46",
   "metadata": {},
   "source": [
    "You can visualize them with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_iris_pairplot\n",
    "\n",
    "visualize_iris_pairplot(iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8133fcf",
   "metadata": {},
   "source": [
    "In Scikit-learn, to apply PCA to a dataset, an instance of the `**PCA**`** **class is created, which must be imported from the `sklearn.decomposition` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb62195",
   "metadata": {},
   "source": [
    "One of the most important hyperparameters of PCA is the number of components we want, this number, given by the `n_components` argument, which I recommend specifying in most cases. Let's say we want to reduce our dataset to only two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e25d4",
   "metadata": {},
   "source": [
    "This means that from 4 dimensions, we are going to convert it to two â€“ by calling the `fit` method and then `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "print(X_reduced.shape)\n",
    "X_reduced[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ef3ee",
   "metadata": {},
   "source": [
    "And now we can visualize this new dataset, which is a low-dimensional version that captures the differences of the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar los datos transformados\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels)\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0c731",
   "metadata": {},
   "source": [
    "Something important to note is that after the transformations, these two new dimensions, these values have no relation to any physical property. They are just \"components,\" here we cannot speak of centimeters or petals, none of that.\n",
    "\n",
    "## How to measure how good PCA is?\n",
    "\n",
    "It is difficult on its own to quantify how good our choice of PCA hyperparameters is. Sometimes the performance of PCA is measured in conjunction with how well it is able to help improve the performance of a machine learning model that is trained with the data coming out of PCA, or if the graphs we generate with it are good or not.\n",
    "\n",
    "And there you have it, the PCA algorithm is useful when we need to reduce the dimension of our data, either to train a new model or simply visualize data.\n",
    "\n",
    "And there you have it, PCA is an algorithm that perhaps on its own its usefulness is not so evident, but when you put it together with a graph or a machine learning model, it begins to gain more importance and its usefulness becomes evident."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
