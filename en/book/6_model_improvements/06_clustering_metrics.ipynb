{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ac3485",
   "metadata": {},
   "source": [
    "# Clustering Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acc593",
   "metadata": {},
   "source": [
    "## Clustering Metrics\n",
    "\n",
    "First, we are going to generate a small dataset to demonstrate the metrics; in this case, it's best to visualize it. Remember that visualization will not always be possible, as your data may have more than 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9806d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generar datos aleatorios y predicciones con KMeans\n",
    "X, y_true = make_blobs(n_samples=1000, centers=4, cluster_std=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4385bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_6 = KMeans(n_clusters=6, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_5 = KMeans(n_clusters=5, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_4 = KMeans(n_clusters=4, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_pred_3 = KMeans(n_clusters=3, random_state=42, n_init='auto').fit_predict(X)\n",
    "y_wrong = np.random.randint(4, size=1000)\n",
    "\n",
    "predichos = [y_pred_3, y_pred_4, y_pred_5, y_pred_6]\n",
    "\n",
    "# Crear los subplots lado a lado\n",
    "fig, axs = plt.subplots(1, 2 + len(predichos), figsize=(25, 5))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c='k', alpha=0.5)\n",
    "axs[0].set_title('Datos originales')\n",
    "\n",
    "for idx, y_preds in enumerate(predichos, 1):\n",
    "    axs[idx].scatter(X[:, 0], X[:, 1], c=y_preds)\n",
    "    axs[idx].set_title(f'{idx+2} clusters encontrados')\n",
    "axs[-1].scatter(X[:, 0], X[:, 1], c=y_wrong)\n",
    "axs[-1].set_title('Mal clusttering')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2c105",
   "metadata": {},
   "source": [
    "### Silhouette score\n",
    "\n",
    "This measures how well the data is clustered and how separated the groups are. This metric takes values between -1 and 1, where 1 indicates perfect clustering, 0 indicates that the groups overlap, and -1 indicates that the points are assigned to the wrong group. Obviously, the result we are hoping for is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45df0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "c3 = silhouette_score(X, y_pred_3)\n",
    "c4 = silhouette_score(X, y_pred_4)\n",
    "c5 = silhouette_score(X, y_pred_5)\n",
    "c6 = silhouette_score(X, y_pred_6)\n",
    "wrong = silhouette_score(X, y_wrong)\n",
    "\n",
    "print(f'Silhouette Score for 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9facf64",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz Index:\n",
    "\n",
    "This measures the separation between groups and the dispersion within groups. The higher the value of this metric, the better the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "\n",
    "c3 = calinski_harabasz_score(X, y_pred_3)\n",
    "c4 = calinski_harabasz_score(X, y_pred_4)\n",
    "c5 = calinski_harabasz_score(X, y_pred_5)\n",
    "c6 = calinski_harabasz_score(X, y_pred_6)\n",
    "wrong = calinski_harabasz_score(X, y_wrong)\n",
    "\n",
    "print(f'Índice Calinski-Harabasz para 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf7eaf",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index\n",
    "\n",
    "This measures the \"compactness\" of each cluster and the separation between clusters. The lower the value of this metric, the better the grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "c3 = davies_bouldin_score(X, y_pred_3)\n",
    "c4 = davies_bouldin_score(X, y_pred_4)\n",
    "c5 = davies_bouldin_score(X, y_pred_5)\n",
    "c6 = davies_bouldin_score(X, y_pred_6)\n",
    "wrong = davies_bouldin_score(X, y_wrong)\n",
    "\n",
    "print(f'Índice Davies-Bouldin para 3: {c3:0.2f}, 4: {c4:0.2f}, 5: {c5:0.2f}, 6: {c6:0.2f} and random: {wrong:0.2f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96508ce3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And there you have it, these were some of the metrics offered by scikit-learn. If you notice, the functions for calculating supervised learning metrics follow a pattern: true values as the first argument and predicted values after. Similarly, for unsupervised learning functions, where you have to pass the input variables as well as the assigned clusters.\n",
    "\n",
    "It's also important to highlight that each metric has its own purpose and there isn't a universally better metric for evaluating a model, as it depends on the problem at hand and the user's needs.\n",
    "\n",
    "In the next chapter, we'll see how we can visualize our metrics to present them to the user or to better understand them ourselves."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
