{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38faab94",
   "metadata": {},
   "source": [
    "# Pipelines (Parte 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9231b3b",
   "metadata": {},
   "source": [
    "Hello again, welcome to the second part of the book chapter on Pipelines with scikit-learn, where I'm going to teach you how to create more complex pipelines and deal with DataFrames.\n",
    "\n",
    "## Composite Pipelines\n",
    "\n",
    "So far, we've seen the usefulness of *pipelines* and how we can use them. But we've created fairly simple pipelines, don't you think?\n",
    "\n",
    "Let's create a slightly more complicated one, but for that, we're going to need a slightly more complicated dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_complex_data\n",
    "\n",
    "dataset = load_complex_data()\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a5ed6",
   "metadata": {},
   "source": [
    "There are 6 columns, one of them is an `ID`, `job`, `marital` are categories, `balance`, `age` and `loyalty` are numerical, and `subscribed`, the target variable, is binary categorical.\n",
    "\n",
    "Let's prepare this dataset.\n",
    "\n",
    "## `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e734bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encode_categories = ColumnTransformer([\n",
    "    (\n",
    "        'one_hot_encode_categories', # Nombre de la transformación\n",
    "        OneHotEncoder(sparse_output=False), # Transformación a aplicar\n",
    "        [\"job\", 'marital'] # Columnas involucradas\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1129e1",
   "metadata": {},
   "source": [
    "Let's see what it does with our dataset after training it with `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode_categories.fit(dataset)\n",
    "\n",
    "transformed_dataset = one_hot_encode_categories.transform(dataset)\n",
    "transformed_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8fcb6",
   "metadata": {},
   "source": [
    "One can access the elements of `ColumnTransformer` with the `named_transformers_` attribute and from there we will access the `categories_` attribute to retrieve the headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92333728",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = one_hot_encode_categories.named_transformers_['one_hot_encode_categories'].categories_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c67b1b",
   "metadata": {},
   "source": [
    "We can use this function that I created to view this matrix as a dataframe with the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_transformed_data\n",
    "\n",
    "show_transformed_data(transformed_dataset, cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524dbf3",
   "metadata": {},
   "source": [
    "## Nested pipelines\n",
    "\n",
    "Let's do something with the `age` variable. The first thing to notice is that the `age` variable has null values, we need to impute its values and then we're going to discretize it, let's make a pipeline for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc9c5b",
   "metadata": {},
   "source": [
    "If we test it by passing the `age` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_pipeline.fit_transform(dataset[['age']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a5fcc",
   "metadata": {},
   "source": [
    "We are going to wrap this pipeline in a column transformer so that it works directly with the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])\n",
    "\n",
    "handle_age_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'handle_age_transformer', # Nombre de la transformación\n",
    "        handle_age_pipeline, # Transformación a aplicar\n",
    "        [\"age\"] # Columnas involucradas\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d203fb",
   "metadata": {},
   "source": [
    "And we can verify that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f85906",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_age_transformer.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb5078",
   "metadata": {},
   "source": [
    "## Leaving variables untransformed\n",
    "\n",
    "You can use the `passthrough` string to let variables pass through without any transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "let_loyalty_pass_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'leave_loyalty_alone',\n",
    "        'passthrough',\n",
    "        ['loyalty']\n",
    "    )\n",
    "])\n",
    "\n",
    "let_loyalty_pass_transformer.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f757c4",
   "metadata": {},
   "source": [
    "## `FeatureUnion` to put it all together\n",
    "\n",
    "Let's recreate everything we just did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b063f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya lo vimos más arriba\n",
    "one_hot_encode_categories = ColumnTransformer([\n",
    "    (\n",
    "        'one_hot_encode_categories', # Nombre de la transformación\n",
    "        OneHotEncoder(sparse_output=False), # Transformación a aplicar\n",
    "        [\"job\", 'marital'] # Columnas involucradas\n",
    "    )\n",
    "])\n",
    "\n",
    "# Ya lo vimos más arriba\n",
    "handle_age_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('discretize', KBinsDiscretizer(encode=\"onehot-dense\"))\n",
    "])\n",
    "handle_age_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'handle_age_transformer', # Nombre de la transformación\n",
    "        handle_age_pipeline, # Transformación a aplicar\n",
    "        [\"age\"] # Columnas involucradas\n",
    "    )\n",
    "])\n",
    "\n",
    "# Ya lo vimos más arriba\n",
    "let_loyalty_pass_transformer = ColumnTransformer([\n",
    "    (\n",
    "        'leave_loyalty_alone',\n",
    "        'passthrough',\n",
    "        ['loyalty']\n",
    "    )\n",
    "])\n",
    "\n",
    "# Este es nuevo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale_balance = ColumnTransformer([\n",
    "    ('scale_balance', StandardScaler(), ['balance'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0877d",
   "metadata": {},
   "source": [
    "Remember that thanks to `ColumnTransformer`, each of these individual transformers acts on only a few columns of the dataset and discards the rest. But in reality, what we want is to generate a single dataset.\n",
    "\n",
    "We can use the `FeatureUnion` class to join our features horizontally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "all_the_features = FeatureUnion([\n",
    "    ('one_hot_encode_categories', one_hot_encode_categories),\n",
    "    ('handle_age_transformer', handle_age_transformer),\n",
    "    ('let_loyalty_pass_transformer', let_loyalty_pass_transformer),\n",
    "    ('scale_balance', scale_balance)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356b364",
   "metadata": {},
   "source": [
    "And if we call `fit_transform`, we will obtain a new transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = all_the_features.fit_transform(dataset)\n",
    "transformed_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917e138",
   "metadata": {},
   "source": [
    "This dataset has 22 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b860f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed79d9",
   "metadata": {},
   "source": [
    "15 of them come from the categorical variables `job`, `marital`, 5 come from the `age` column that we binarized, and then `balance` and `loyalty` are the two remaining ones. And well, in the process we got rid of the `ID` column which is useless for us in this case.\n",
    "\n",
    "## Training a model\n",
    "\n",
    "To finish, we're going to add a machine learning model at the end to be the crown jewel and have everything in one place.\n",
    "\n",
    "First, we're going to use `clone` to create untrained copies of our entire pipeline already created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "feature_transformer = clone(all_the_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b8736",
   "metadata": {},
   "source": [
    "We create the final pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "inference_pipeline = Pipeline([\n",
    "    ('featurize', feature_transformer),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6be9d0",
   "metadata": {},
   "source": [
    "To visualize what is happening, you can simply display it by leaving it alone in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492060a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d792d",
   "metadata": {},
   "source": [
    "Now, let's train it like any other estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c79cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.fit(\n",
    "    dataset,\n",
    "    dataset['subscribed']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f11152",
   "metadata": {},
   "source": [
    "And if we create a new example, we can execute predict without any problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b58dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nuevos_datos = pd.DataFrame([\n",
    "    {\n",
    "        \"ID\": 2432,\n",
    "        \"job\": \"technician\",\n",
    "        \"marital\": \"single\",\n",
    "        \"balance\": 90,\n",
    "        \"age\": 34,\n",
    "        \"loyalty\": 0.5\n",
    "    }\n",
    "])\n",
    "\n",
    "nuevos_datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline.predict(nuevos_datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcb3e9",
   "metadata": {},
   "source": [
    "And that's it, now all you need to store and share is the `inference_pipeline` object!\n",
    "\n",
    "## When to use them and when not to?\n",
    "\n",
    "As you can see, pipelines are very useful in many cases and offer various advantages. However, there are situations where they are not the best option. Here are some general tips on when to use or not use pipelines:\n",
    "\n",
    "### **When to use pipelines:**\n",
    "\n",
    " 1. Sequential processing: If your machine learning workflow follows a sequential structure, pipelines are ideal for organizing and simplifying the process.\n",
    " 1. Cross-validation and hyperparameter tuning: Pipelines facilitate cross-validation and hyperparameter tuning, ensuring that data transformations are applied consistently and avoiding problems such as data leakage.\n",
    " 1. Reproducibility and maintainability: If you want to improve the reproducibility and maintainability of your code, pipelines are an excellent option, as they allow you to encapsulate the entire workflow in a single structure.\n",
    " 1. Project collaboration: If you're working in a team, pipelines can facilitate collaboration by providing a clear and coherent representation of the different stages of the machine learning process.\n",
    "\n",
    "### **When not to use pipelines:**\n",
    "\n",
    " 1. Complex preprocessing: If your dataset requires operations that cannot be easily represented as scikit-learn transformers, pipelines may not be suitable.\n",
    " 1. Custom workflows: If you need to make transformations that don't fit into the sequential structure of a scikit-learn pipeline, you may need to handle the steps manually.\n",
    " 1. Models outside of scikit-learn: If you're using machine learning models or tools from other libraries that don't follow the scikit-learn API, you may not be able to use a pipeline directly.\n",
    " 1. If you're dealing with enormous amounts of data: it may sometimes be better to carry out data transformations in other languages, such as SQL to save time.\n",
    "\n",
    "In summary, scikit-learn pipelines are a powerful tool for many machine learning workflows, but they may not be suitable for all situations. Consider the specific needs and limitations of your project before deciding if a pipeline is the best option.\n",
    "\n",
    "See you in the next chapter where we'll discover how to save our models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
