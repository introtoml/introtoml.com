{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16716df",
   "metadata": {},
   "source": [
    "# Trabajando con texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dba20",
   "metadata": {},
   "source": [
    "Welcome to this chapter of your machine learning book with scikit-learn.\n",
    "\n",
    "You've probably heard of something called ChatGPT. Well, here are its principles: in text analysis.\n",
    "\n",
    "Text analysis is a topic that probably deserves its own book, but we can start laying the foundations using scikit-learn.\n",
    "\n",
    "First, it's important to remember that written words are not directly consumable by machine learning models; we need to convert them into a numerical representation that we can process.\n",
    "\n",
    "## Bag of words ‚Äì CountVectorizer\n",
    "\n",
    "One of the most common ways to do this is by using the bag of words (BoW) technique, which converts text into a word frequency matrix.\n",
    "\n",
    "To accomplish this, scikit-learn offers us a utility that performs the following for us:\n",
    "\n",
    " - Splits the text into tokens, generally complete words,\n",
    " - Counts the occurrences of each of these tokens,\n",
    " - Assigns values within a vector according to the number of occurrences of each token in our input data.\n",
    "This is done through the vectorizer known as `CountVectorizer`. To see it in action, we first need to generate a dataset. By the way, a dataset is often called a corpus, and each of its individual elements is known as a document.\n",
    "\n",
    "So let's generate a corpus with three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa527b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Scikit-learn nos ayuda a trabajar con texto\",\n",
    "    \"Parte el texto en tokens, generalmente palabras completas\",\n",
    "    \"Cuenta las ocurrencias de cada uno de estos tokens\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe762b3",
   "metadata": {},
   "source": [
    "We import the vectorizer ‚Äì note that we are importing from `sklearn.feature_extraction.text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244caf7",
   "metadata": {},
   "source": [
    "We create an object with default values ‚Äì and train it with our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d531e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdd55",
   "metadata": {},
   "source": [
    "If we call the `transform` method passing our corpus to it, the result is as expected: a sparse matrix since that is the best representation of our data. We can convert it to a NumPy array using its `todense` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_corpus = count_vectorizer.transform(corpus)\n",
    "transformed_corpus.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820026c7",
   "metadata": {},
   "source": [
    "Yes, it's a matrix with a lot of zeros. If you want to see which word corresponds to each column, you can access the computed property `vocabulary_`, so you can see that the word \"help\" corresponds to column zero and the word \"one\" corresponds to column number 20.\n",
    "\n",
    "## Inverse transformation\n",
    "\n",
    "The `CountVectorizer` offers us the `inverse_transform` method so that from a matrix of vectors, you can recover the tokens you used at the input. Be careful because although it is an inverse transformation, it is not so reliable, since one of the disadvantages of this family of vectorizers is that the order of words is lost.\n",
    "\n",
    "We can test it by calling the method with the matrix we just obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01313f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.inverse_transform(transformed_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1b908",
   "metadata": {},
   "source": [
    "## Extra Parameters\n",
    "\n",
    "The `CountVectorizer` is one of the first classes that has a large number of parameters to configure its behavior. Among the most common ones I've seen used are:\n",
    "\n",
    " - `binary`, which by default has a value of `False`. When this value is true, the `CountVectorizer` behaves more like a one-hot encoder, and our resulting matrix consists of ones and zeros.\n",
    " - `max_features`, this can be a number that indicates the maximum number of columns we want in our matrix. In the previous example, we had a matrix of 21 columns as a result, but if we had set `max_features` to a value of 10, we would have a matrix of 10 columns as a result, where those ten columns would contain the 10 most frequent tokens.\n",
    " - `max_df` and `min_df`, these parameters allow us to eliminate words that are overrepresented and underrepresented in our corpus. These values can be floats, ranging from 0 to 1 if we want to use them as a proportion, or we can use them as integers if we want to count occurrences directly.\n",
    "For example, if we create a vectorizer with the following arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_count_vectorizer = CountVectorizer(\n",
    "    binary = True,\n",
    "    max_features = 6,\n",
    "    min_df = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233850c5",
   "metadata": {},
   "source": [
    "You will see that as a result of transforming our corpus, we obtain a matrix of 6 columns, filled with only ones and zeros ‚Äì and that the vocabulary is composed solely of the six most frequent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = modified_count_vectorizer.fit_transform(corpus)\n",
    "print(modified_count_vectorizer.vocabulary_)\n",
    "new_result.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cdbdbf",
   "metadata": {},
   "source": [
    "## Changing the tokenizer\n",
    "\n",
    "There are times when we want to have more control over how the documents in our corpus should be broken down into tokens. For example, if we are dealing with another language, or we are working with text that contains emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Un tokenizador que mantiene solo los emojis\n",
    "def emoji_tokenizer(text):\n",
    "    emojis = re.findall(r'[\\U0001F000-\\U0001F6FF]', text)\n",
    "    return emojis\n",
    "\n",
    "print(emoji_tokenizer(\"I üíö üçï\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149e4c6",
   "metadata": {},
   "source": [
    "We use the tokenizer by passing it to `CountVectorizer` in the `tokenizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_corpus = [\n",
    "    \"I üíö üçï\",\n",
    "    \"This üçï was üëé\",\n",
    "    \"I like either üçï or üçî, but not üå≠\",\n",
    "]\n",
    "\n",
    "emoji_vectorizer = CountVectorizer(tokenizer=emoji_tokenizer)\n",
    "X = emoji_vectorizer.fit_transform(emoji_corpus)\n",
    "\n",
    "# Print the feature names and the count matrix\n",
    "print(emoji_vectorizer.vocabulary_)\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cc480",
   "metadata": {},
   "source": [
    "## TF-IDF Weighting ‚Äì TfidfVectorizer\n",
    "\n",
    "In a large text corpus, some words will be heavily overrepresented (such as \"the\", \"a\", \"is\") and therefore have little meaningful information about the actual content of the document ‚Äì if these words exist in all documents, they are not as useful.\n",
    "\n",
    "If we take the results of a `CountVectorizer` in these cases, we run the risk of passing unnecessary information to our model, thus obscuring less frequent, rarer, and more interesting words. To address this problem, we can use text feature extraction techniques that weight words based on their relative importance in the corpus.\n",
    "\n",
    "A common technique used for text feature extraction is term frequency ‚Äì inverse document frequency or (TF-IDF), which measures the relative importance of a word in a document based on the frequency of that word in the corpus as a whole.\n",
    "\n",
    "Scikit-learn offers us a class called `TfidfVectorizer` that has the same external behavior as the `CountVectorizer`: it receives a set of texts and gives us a corresponding matrix. It also has almost the same arguments.\n",
    "\n",
    "```{hint} \n",
    "As homework, try the examples we saw but using the `TfidfVectorizer`, tell me in the comments what differences you notice?\n",
    "\n",
    "```\n",
    "## Feature hashing\n",
    "\n",
    "Another way to convert text to numerical representation is through the use of the *hashing trick*, remember we saw it recently? ‚Äì for this case, scikit-learn has another class for us: `HashingVectorizer`, which shares many characteristics with the two vectorizers previously seen. Obviously, it has the same limitations that we already know, however it can be a good alternative in some scenarios ‚Äì ah, remember, using *feature hashing* it's impossible to return to the original values.\n",
    "\n",
    "```{hint}\n",
    "Another homework: try the examples we saw but using the `HashingVectorizer`, tell me in the comments what differences you notice?\n",
    "\n",
    "```\n",
    "## Conclusion\n",
    "\n",
    "We've seen multiple ways to transform our text into numbers, `CountVectorizer` in count mode or binary mode, `TfidfVectorizer` and `HashingVectorizer`. The method you should use will depend on your use case, but you can follow these general rules:\n",
    "\n",
    " - `**CountVectorizer**` is useful for creating a word count matrix when the absolute number of occurrences of each word in the text is important.\n",
    " - `**TfidfVectorizer**` is useful when weighting the importance of each word in the text based on how often it appears in the corpus.\n",
    " - `**HashingVectorizer**` is useful for working with very large datasets that don't fit in memory and for reducing the dimensionality of the feature space.\n",
    "I hope now it's clearer to you what the first step is before trying to feed text into your machine learning models, remember that you must practice and in the resources you'll find practical examples where some vectorizers are used.\n",
    "\n",
    "In the following chapters we'll see how we can work with categorical and numerical variables. I'll see you there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8644d6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
