{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf3f4b0",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0970f37",
   "metadata": {},
   "source": [
    "You already know that there is a regression that, instead of predicting continuous numerical values, helps us make classifications. This regression is logistic regression, and in scikit-learn, the implementation of this algorithm is found in the `LogisticRegression` class.\n",
    "\n",
    "Logistic regression is a supervised learning model commonly used for classification problems.\n",
    "\n",
    "Given a set of features, logistic regression estimates the probability that an instance belongs to a particular class. This probability is transformed into a class label using a decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06294465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, random_state=42, noise=0.40)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239922a",
   "metadata": {},
   "source": [
    "Then, an object of the `LogisticRegression` class is instantiated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0292b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054526a",
   "metadata": {},
   "source": [
    "And it is adjusted to the training data using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead013a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ea766",
   "metadata": {},
   "source": [
    "Once trained, the model can be used to make predictions on the test data using the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c750627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81125330",
   "metadata": {},
   "source": [
    "The truth is that there isn't much science to it.\n",
    "\n",
    "## Predict proba\n",
    "\n",
    "In classification problems, scikit-learn classifiers have a method called `predict_proba` that you can use to obtain an estimate of how likely an instance is to belong to one class or another.\n",
    "\n",
    "For example, you can call the predict proba method on our model and our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = lr.predict_proba(X_test)\n",
    "probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d8124",
   "metadata": {},
   "source": [
    "In this case, as we are talking about a binary classification problem, `probabilities` is a matrix with two columns, where the first column represents the probability that the sample belongs to the negative class and the second to the positive class.\n",
    "\n",
    "Predicting probabilities instead of obtaining a hard classification is useful in some cases. To learn more, I invite you to check the resources in this book.\n",
    "\n",
    "Remember also that all scikit-learn classifiers have this method, not just linear regression.\n",
    "\n",
    "## Arguments\n",
    "\n",
    "The `LogisticRegression` class in scikit-learn has a large number of parameters that allow customizing the model according to the specific needs of the problem. Here are some common ones that I recommend you experiment with when working:\n",
    "\n",
    " - `penalty`: specifies the regularization norm to use in the model. Common options are \"L1\", \"L2\", and \"elasticnet\". The default value is \"L2\". In general, my recommendation is to try not to use \"L1\" with logistic regression. \n",
    " - `tol`: specifies the tolerance for detecting convergence of the optimization algorithm. As this is an iterative algorithm, it's important to establish a tolerance value, in case the algorithm reaches a point where the values don't change enough, to be able to stop the training.\n",
    " - `max_iter`: continuing on the topic of iterations, it's also possible to set a maximum number of these.\n",
    " - `C`: is a value that controls the strength with which regularization is applied. `C` has the peculiarity of being a value that inversely affects regularization; the smaller this value, the stronger the applied regularization.\n",
    " - `class_weight`: this argument is useful when you're dealing with a problem where there's an imbalance in the data.  \n",
    "## Example of using `class_weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4712511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import classification_report_comparison\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Create an imbalanced classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n",
    "                           weights=[0.9], random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Fit Logistic Regression with class_weight='balanced'\n",
    "balanced_lr = LogisticRegression(class_weight='balanced')\n",
    "balanced_lr.fit(X_train, y_train)\n",
    "\n",
    "vanilla_lr = LogisticRegression()\n",
    "vanilla_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "balanced_y_pred = balanced_lr.predict(X_test)\n",
    "vanilla_y_pred = vanilla_lr.predict(X_test)\n",
    "\n",
    "classification_report_comparison(y_test, {\"Balanced\": balanced_y_pred, \"No balance\": vanilla_y_pred})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a5f3d",
   "metadata": {},
   "source": [
    "```{hint} As an exercise, why don't you try playing a bit more with the parameters? Use the `classification_report_comparison` function\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
