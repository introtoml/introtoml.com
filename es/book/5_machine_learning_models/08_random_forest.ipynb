{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6c68d1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbc896",
   "metadata": {},
   "source": [
    "Hello again. Welcome to the machine learning book with scikit-learn, it's time to continue talking about classification algorithms.\n",
    "\n",
    "When it comes to classification, another popular algorithm is *random forest classifier*, this algorithm is composed of several decision trees that in turn vote to choose the classification of a certain element.\n",
    "\n",
    "Random forests are implemented in the `RandomForestClassifier` class of the `sklearn.ensemble` module, and the way to use it is no different from other models in Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=15, n_classes=2, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935adab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26235cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc.predict(X_test))\n",
    "rfc.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ff2094",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "\n",
    "The interesting part are the arguments of the class:\n",
    "\n",
    " - `n_estimators`: the number of trees in the forest. A higher value can improve the model's accuracy, but increases training time and memory usage.\n",
    " - `max_depth`: the maximum depth of each tree in the forest. A higher value can increase the model's ability to fit the training data, but can also cause overfitting.\n",
    " - `min_samples_split`: the minimum number of samples required to split an internal node. A lower value can allow the model to capture more complex relationships between variables, but can also increase the risk of overfitting.\n",
    " - `min_samples_leaf`: the minimum number of samples required in each leaf of the tree. A higher value can prevent the model from overfitting to the training data, but can also reduce the model's ability to capture complex relationships between variables.\n",
    " - `max_features`: the maximum number of variables considered when splitting a node. A lower value can reduce overfitting, but can also reduce the model's ability to capture complex relationships between variables.\n",
    "## Behavior of some arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63534dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=42, noise=0.10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd1e54",
   "metadata": {},
   "source": [
    "### `n_estimators` - number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_boundaries\n",
    "\n",
    "plot_boundaries(\n",
    "    X, y, \n",
    "    [\n",
    "        ('n_estimators = 1', RandomForestClassifier(n_estimators=1)),\n",
    "        ('n_estimators = 10', RandomForestClassifier(n_estimators=10)),\n",
    "        ('n_estimators = 100', RandomForestClassifier(n_estimators=100)),\n",
    "        ('n_estimators = 1000', RandomForestClassifier(n_estimators=1000)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79222bb",
   "metadata": {},
   "source": [
    "### `max_depth` - maximum depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(\n",
    "    X, y, \n",
    "    [\n",
    "        ('max_depth = None', RandomForestClassifier(max_depth=None)),\n",
    "        ('max_depth = 1', RandomForestClassifier(max_depth=1)),\n",
    "        ('max_depth = 10', RandomForestClassifier(max_depth=10)),\n",
    "        ('max_depth = 100', RandomForestClassifier(max_depth=100)),\n",
    "        ('max_depth = 1000', RandomForestClassifier(max_depth=1000)),\n",
    "        ('max_depth = 10000', RandomForestClassifier(max_depth=10000)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab6dac",
   "metadata": {},
   "source": [
    "### `min_samples_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebbd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(\n",
    "    X, y, \n",
    "    [\n",
    "        ('min_samples_split = 2', RandomForestClassifier(min_samples_split=2)),\n",
    "        ('min_samples_split = 10', RandomForestClassifier(min_samples_split=10)),\n",
    "        ('min_samples_split = 20', RandomForestClassifier(min_samples_split=20)),\n",
    "        ('min_samples_split = 30', RandomForestClassifier(min_samples_split=30)),\n",
    "        ('min_samples_split = 40', RandomForestClassifier(min_samples_split=40)),\n",
    "        ('min_samples_split = 50', RandomForestClassifier(min_samples_split=50)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d44d3b",
   "metadata": {},
   "source": [
    "## Number of estimators\n",
    "\n",
    "A random forest is nothing more than a set of decision trees, each with small variations. When it's time to classify a new instance, each of these trees casts its vote and in the end, the class with the most votes wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abf69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un RandomForestClassifier con un solo estimador\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(iris.data, iris.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6b35b",
   "metadata": {},
   "source": [
    "As we specified it to have 100 estimators, that is exactly the amount it has in the `estimators_` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rfc.estimators_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636a098",
   "metadata": {},
   "source": [
    "And what's even better, we can visualize each tree individually using the `plot_tree` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa869bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_tree(rfc.estimators_[0], feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4e049",
   "metadata": {},
   "source": [
    "Examining trees one by one is not actually realistic, but it is still interesting and gives us a perspective on what is happening inside the classifier.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Another interesting thing that can be done is related to another attribute: `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d18a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba4fcf",
   "metadata": {},
   "source": [
    "This is an array where each of the entries corresponds to the features used to train the model.\n",
    "\n",
    "The array by itself doesn't say much, but we can plot them using a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(iris.feature_names, rfc.feature_importances_)\n",
    "ax.set_title(\"Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee328e87",
   "metadata": {},
   "source": [
    "This array and the subsequent graph allow us to visualize how much each variable contributes to the model in making its predictions. This can help us select the most important features if we want to reduce the dimensionality of our dataset.\n",
    "\n",
    "It can also provide us with tools to interpret how the model is making predictions and try to understand its behavior.\n",
    "\n",
    "And there you have it, we've seen some of the properties and arguments of random forests."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
