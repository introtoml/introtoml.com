{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e49f85",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410eb7f",
   "metadata": {},
   "source": [
    "Hello, welcome to this chapter on Support Vector Classifiers with Scikit-Learn.\n",
    "\n",
    "SVM is an algorithm that helps us perform data classification. You already know that the basic operation of SVM consists of finding a hyperplane that can separate different categories of data,\n",
    "\n",
    "Scikit-Learn provides us with an implementation of SVM for classification in the `SVC` class within the `sklearn.svm` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5286f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa3a4f",
   "metadata": {},
   "source": [
    "But first, let's generate a dataset to test the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=15, n_classes=2, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad474d",
   "metadata": {},
   "source": [
    "And then we can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a171db",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb1a45",
   "metadata": {},
   "source": [
    "And we can evaluate the performance using the accuracy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisi√≥n del modelo SVM: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190cebc",
   "metadata": {},
   "source": [
    "## Function Arguments\n",
    "\n",
    "The SVC class has a large number of arguments, and you already know that their importance varies from problem to problem. However, the first ones you should consider adjusting are the following:\n",
    "\n",
    " - `C`: This parameter controls the penalty for classification errors. An appropriate value of C can help avoid both *overfitting* and *underfitting*.\n",
    " - `kernel`: This parameter determines the transformation that will be applied to the data before executing the SVC algorithm. I recommend trying several to find the best one for your model - particularly if they are not linearly separable.\n",
    " - `degree`: This parameter is used with the `poly` kernel and controls the degree of the polynomial used for data transformation. A degree greater than 1 provides a more complex boundary but runs the risk of *overfitting*.\n",
    " - `gamma`: This parameter is used with the `poly`, `rbf`, and `sigmoid` kernels and controls how they behave. An appropriate value of gamma can help avoid both *overfitting* and *underfitting*.\n",
    " - `class_weight`: This parameter is used to address class imbalance in the dataset. If the dataset has imbalanced classes, adjusting class weights can significantly improve model performance. We have already seen the importance of this argument in classification problems in the chapter on logistic regression.\n",
    "\n",
    "## Behavior of Some Arguments\n",
    "\n",
    "Let's look at the effect that some arguments have on SVC by visualizing the decision boundary.\n",
    "\n",
    "First, we need to create a toy dataset with two dimensions or features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fddd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=42, noise=0.10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21c359",
   "metadata": {},
   "source": [
    "### `C` - Regularization\n",
    "\n",
    "In a way, we can think of this value as one that allows us to choose how strict the margin between the line and the elements of our dataset should be. Here you should keep in mind that the lower the value, the less effect it will have on the margin - by default, the value is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89104816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_boundaries\n",
    "\n",
    "C_values = [0.001, 0.01,0.1, 1, 10]\n",
    "svcs = []\n",
    "\n",
    "for C_value in C_values:\n",
    "    svcs.append(\n",
    "        (f\"C = {C_value}\", SVC(kernel='linear', C=C_value))\n",
    "    )\n",
    "\n",
    "plot_boundaries(X, y, svcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4006e7",
   "metadata": {},
   "source": [
    "## `kernel`\n",
    "\n",
    "This argument controls the internal transformation applied to the data to attempt to obtain a separating hyperplane. By default, the value is RBF, which stands for *radial basis function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f310990",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "svcs = []\n",
    "for kernel in kernels:\n",
    "    svcs.append(\n",
    "        (f\"Kernel = {kernel}\", SVC(kernel=kernel, C=1))\n",
    "    )\n",
    "plot_boundaries(X, y, svcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c4c41",
   "metadata": {},
   "source": [
    "## `degree`\n",
    "\n",
    "This controls the degree of the polynomial when the `poly` kernel is chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_values = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "svcs = [\n",
    "    (f\"Degree = {degree}\", SVC(kernel='poly', degree=degree, C=1))\n",
    "    for degree in degree_values\n",
    "]\n",
    "\n",
    "plot_boundaries(X, y, svcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055feba3",
   "metadata": {},
   "source": [
    "## `LinearSVC`\n",
    "\n",
    "There is another class within Scikit-Learn's `svm` module called `LinearSVC` that you should consider if your dataset is large or has many features. `LinearSVC` is a version of SVM that uses a linear kernel and is optimized for linear classification. It can be much faster on large datasets than SVC with a non-linear kernel, and it consumes less memory.\n",
    "\n",
    "The difference between `SVC` and `LinearSVC` is that `SVC` can use any kernel (e.g., linear, polynomial, RBF), while `LinearSVC` can only use a linear kernel. As a result, `SVC` can be more powerful and accurate than `LinearSVC` for non-linear problems, but it can also be slower and consume more memory. On the other hand, `LinearSVC` is faster and consumes less memory, but can only solve linear problems.\n",
    "\n",
    "In general, it's a good idea to try both options and different configurations to find the best combination that works for your needs, always keeping in mind the cost-benefit trade-off between them.\n",
    "\n",
    "And there you have it. I hope that now you have a clearer understanding of the types of problems where you can apply SVM and what the most important arguments are to start tuning the hyperparameters. The next chapter of the book will discuss more classification algorithms."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
