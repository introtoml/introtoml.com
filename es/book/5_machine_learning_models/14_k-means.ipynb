{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d353c17",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534dc8cd",
   "metadata": {},
   "source": [
    "Hello, welcome to this chapter of your book on machine learning with Scikit Learn. This covers unsupervised learning, that is, clustering and dimensionality reduction. The first lesson is about the k-Means algorithm.\n",
    "\n",
    "The k-means algorithm is a clustering or data grouping method used to classify unlabeled datasets into groups or clusters based on the similarity of their characteristics.\n",
    "\n",
    "It is considered unsupervised learning because it does not need labels to function.\n",
    "\n",
    "In Scikit-learn, the implementation of k-means is found in the `**KMeans**` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5fd0e",
   "metadata": {},
   "source": [
    "This book offers several configuration options, such as the number of clusters to be found, the initialization of centroids, and the maximum number of iterations. But before we look at them, I'll teach you its basic usage with the Iris dataset (remember that these are three types of flowers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152373de",
   "metadata": {},
   "source": [
    "To initialize `KMeans`, it is necessary to specify the number of clusters we want to obtain, and this is perhaps one of the algorithm's weaknesses: you have to specify in advance how many clusters you need – you already know that scikit-learn has default values for its arguments, the default value for this argument is 8, but we are going to leave it at 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179030de",
   "metadata": {},
   "source": [
    "```{hint} Additionally, remember that for k-Means to work correctly, the data must be on similar scales, which means you should try to scale your data before introducing it to the model.\n",
    "\n",
    "```\n",
    "## Attributes\n",
    "\n",
    "Once trained, we can find the clusters of the data using the `labels_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0816170",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51258a97",
   "metadata": {},
   "source": [
    "We can also access the calculated centroids; remember that there are as many centroids as there are clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cc0aa",
   "metadata": {},
   "source": [
    "In this case, we have 3 centroids of 4 dimensions each because our input data was 4-dimensional.\n",
    "\n",
    "However, it is better visualized in a graph (this graph only uses a pair of the dataset's features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import view_centroids_iris\n",
    "\n",
    "view_centroids_iris(kmeans, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde085f",
   "metadata": {},
   "source": [
    "Another attribute is inertia. Inertia measures the internal dispersion of clusters, that is, how far points are from the nearest centroid. In general, the objective of k-Means is to minimize this value. Once trained, we can access this information through the attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914cd8e",
   "metadata": {},
   "source": [
    "## Arguments of `kmeans`\n",
    "\n",
    "The KMeans algorithm has several important arguments that can be adjusted to obtain the desired results. Below, I present the most important arguments:\n",
    "\n",
    " - `**n_clusters**`: Specifies the number of clusters desired in the solution. This is the most important parameter and must be carefully adjusted.\n",
    " - `**init**`: Specifies the initialization method for the cluster centroids. The options are \"k-means++\", \"random\", and a custom array. \"k-means++\" is the default method and is recommended for most cases.\n",
    " - `**n_init**`: Specifies the number of times the algorithm will run with different centroid initializations. The final solution will be the best of all runs. The default value is 10, but it can be increased if you want to find a more precise solution.\n",
    " - `**max_iter**`: Specifies the maximum number of iterations allowed before the algorithm stops, even if it has not converged. The default value is 300.\n",
    " - `**tol**`: Specifies the tolerance for convergence. If the distance between the centroid and its previous centroid is less than `**tol**`, the algorithm is considered to have converged. The default value is `1e-4`.\n",
    "## Playing with the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_centroids\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=6, cluster_std=1, random_state=42)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af05e6",
   "metadata": {},
   "source": [
    "### `n_clusters`\n",
    "\n",
    "Perhaps the most important values to tune are the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e67ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variando n_clusters\n",
    "n_clusters_list = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "trained_kmeans = []\n",
    "titles = []\n",
    "for n_clusters in n_clusters_list:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    kmeans.fit(X)\n",
    "    trained_kmeans.append(kmeans)\n",
    "    titles.append(f\"n_clusters = {n_clusters}\")\n",
    "plot_centroids(input_features=X, trained_kmeans=trained_kmeans, titles=titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bba15e",
   "metadata": {},
   "source": [
    "### Elbow method - the elbow rule\n",
    "\n",
    "Most of the time, it's impossible to visualize the centroids of our data (due to high dimensionality). But you can use the \"elbow rule\". The elbow rule is a heuristic used to determine the optimal number of clusters. It involves looking for an inflection point where the inertia stops changing dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5717abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = [kmeans.inertia_ for kmeans in trained_kmeans]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(n_clusters_list, inertias , marker='o')\n",
    "ax.set_xlabel('Número de clusters')\n",
    "ax.set_ylabel('Inercia')\n",
    "ax.set_title('Regla del codo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e5ce2",
   "metadata": {},
   "source": [
    "Additionally, remember that there are other metrics that we previously saw in the clustering metrics chapter.\n",
    "\n",
    "### `init`\n",
    "\n",
    "The method of initializing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_methods = ['k-means++', 'random', np.array([[-10, -10] for _ in range(6)])]\n",
    "init_titles = ['k-means++', 'random', 'custom']\n",
    "\n",
    "trained_kmeans = []\n",
    "titles = []\n",
    "for title, init in zip(init_titles, init_methods):\n",
    "    kmeans = KMeans(n_clusters=6, init=init, random_state=42, n_init=1)\n",
    "    kmeans.fit(X)\n",
    "    trained_kmeans.append(kmeans)\n",
    "    titles.append(f\"init = {title}\")\n",
    "plot_centroids(input_features=X, trained_kmeans=trained_kmeans, titles=titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a7496",
   "metadata": {},
   "source": [
    "## `max_iter`\n",
    "\n",
    "The maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ee1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variando max_iter\n",
    "max_iter_list = [1, 2, 3, 4, 5, 300]\n",
    "\n",
    "initial_centroids = np.array([[0, 0] for _ in range(6)])\n",
    "\n",
    "trained_kmeans = []\n",
    "titles = []\n",
    "for max_iter in max_iter_list:\n",
    "    kmeans = KMeans(n_clusters=6, max_iter=max_iter, init=initial_centroids, n_init=1, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    trained_kmeans.append(kmeans)\n",
    "    titles.append(f'max_iter: {max_iter}')\n",
    "    \n",
    "plot_centroids(input_features=X, trained_kmeans=trained_kmeans, titles=titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeefa92",
   "metadata": {},
   "source": [
    "## Kmeans and large datasets\n",
    "\n",
    "Kmeans is an algorithm that works well for datasets of moderate size. However, it becomes very inefficient when used for large datasets, both in terms of number of rows or observations, and in number of columns or features.\n",
    "\n",
    "Within Scikit-Learn, there is another algorithm called Mini-batch k-Means, which instead of operating on the entire dataset at once (as is the case with kMeans) operates on a subset of elements at a time.\n",
    "\n",
    "```{hint} As a task, why don't you use it and see its behavior? You can import it from `sklearn.cluster` as `MiniBatchKMeans`.\n",
    "\n",
    "```\n",
    "## In conclusion\n",
    "\n",
    "KMeans is an algorithm that you can use when:\n",
    "\n",
    " 1. You need to group unlabeled data based on their similarity, as KMeans seeks to divide the dataset into compact and separate groups (clusters).\n",
    " 1. You have a dataset of moderate size and not very high dimensionality\n",
    " 1. You want an algorithm that is easy to implement and understand\n",
    "But you should be careful using it with:\n",
    "\n",
    " 1. Data with noise, outlier values, or data that overlaps between different groups\n",
    " 1. High-dimensional data, as KMeans can be affected by the \"curse of dimensionality\"\n",
    " 1. Extremely large datasets, in which case you might consider using Mini-Batch KMeans or other more scalable clustering algorithms.\n",
    " 1. Situations where you don't have a rough idea of the number of clusters, as KMeans requires you to specify the number of clusters beforehand.\n",
    "I'll see you in the next chapter where we'll discuss another clustering algorithm."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
