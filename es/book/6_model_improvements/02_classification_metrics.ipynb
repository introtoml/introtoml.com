{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0cff05",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47966352",
   "metadata": {},
   "source": [
    "Hello again, welcome to this chapter on metrics in scikit-learn.\n",
    "\n",
    "As part of all the tools that scikit-learn offers us, it also provides a way to measure how good the predictions made by the models are.\n",
    "\n",
    "Metrics play a crucial role in the evaluation and selection of machine learning models. These metrics allow us to quantify the performance of our models in terms of accuracy, robustness, and generalization. At the same time, they also allow us to communicate the performance of the models to others outside our machine learning team.\n",
    "\n",
    "scikit-learn offers more than 30 metrics, which are too many to cover in a single chapter, but the concepts and code you will see in this chapter can be applied to all of them since they follow a very similar interface.\n",
    "\n",
    "## Classification metrics\n",
    "\n",
    "First, let's create a small dataset that we'll use to test the classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array(\n",
    "    [0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
    ")\n",
    "\n",
    "y_true = np.array(\n",
    "    [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a0fc5",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\frac{{Verdaderos\\ positivos}\\ +\\ {Verdaderos\\ negativos}}{NÃºmero\\ total\\ de\\ predicciones}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abbd52",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is a measure that represents the proportion of correct predictions out of the total predictions made. It's an easy-to-understand metric, but it can be misleading in situations where the data is imbalanced.\n",
    "\n",
    "The formula is as follows:\n",
    "\n",
    "Which means it will count the number of matches between ones and zeros and divide them by the total amount.\n",
    "\n",
    "To use it in scikit-learn, you need to import it from `sklearn.metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05471310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be399ef",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\frac{{Verdaderos\\ positivos}}{{Verdaderos\\ positivos}\\ +\\ {Falsos\\ positivos}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48c103",
   "metadata": {},
   "source": [
    "The closer the result is to one, the better. Although remember that in certain problems, accuracy is not the best metric to evaluate your model's performance.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision, also known as positive predictive value, is used to measure the model's ability to predict positive cases. Formally, it is defined as the number of true positives divided by the sum of true positives and false positives. Here, a value close to 1 is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21642be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544a5ad",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\frac{{Verdaderos\\ positivos}}{{Verdaderos\\ positivos}\\ +\\ {Falsos\\ negativos}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066488e",
   "metadata": {},
   "source": [
    "Precision metrics are especially useful in situations where the cost of a false positive is high. That is, when it's more important to avoid false positives than false negatives. For example, think about a system that detects spam messages; here, precision is the most important appropriate measure since it's crucial to minimize the number of false positives, in this case legitimate emails identified as spam. Failure to do so could cause our users to lose important information.\n",
    "\n",
    "### Recall\n",
    "\n",
    "The *recall* metric, also known as sensitivity or true positive rate, is used to measure the model's ability to identify all positive cases. Formally, it's defined as the number of true positives divided by the sum of true positives and false negatives. Here, a value close to 1 is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce058aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac64adf",
   "metadata": {},
   "source": [
    "$$\n",
    "  2\\ \\times\\ \\frac{{Precision}\\ \\times\\ {Recall}}{{Precision}\\ +\\ {Recall}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed07ff",
   "metadata": {},
   "source": [
    "Unlike precision, the recall metric is especially useful in situations where the cost of a false negative is high. That is, when it is more important to avoid false negatives than false positives. Think about early cancer detection; it is important to minimize the number of false negatives, cancer patients who are not identified, as this can delay treatment and put the person's life at risk. In this case, recall is more important than precision.\n",
    "\n",
    "### F1 score\n",
    "\n",
    "The F1 score, also known as F1 measure, is a commonly used metric to evaluate the quality of a classification model. It is a single measure that combines precision and recall into a single number.\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, its values range between 0 and 1, with 1 being the best possible result.\n",
    "\n",
    "The formula that represents it is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c12ca",
   "metadata": {},
   "source": [
    "As F1 is a combination of the two metrics, it can be used in various scenarios to measure the overall performance of the model. It's ideal when a balance between being precise and retrieving all possible cases must be found. In the end, the metric to choose depends entirely on the problem you're trying to solve."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
