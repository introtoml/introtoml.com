{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd16b4a",
   "metadata": {},
   "source": [
    "# Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8754012",
   "metadata": {},
   "source": [
    "Hello, it's great to see you here. On this occasion, we're going to talk a bit about visualizing our model's performance.\n",
    "\n",
    "Data visualization is not only done before training a model during exploratory data analysis, but it's also a very useful tool for understanding the performance of our models in a more intuitive and easy-to-interpret way. Fortunately, scikit-learn also offers us several functions to visualize the results of our models.\n",
    "\n",
    "The visualizations I'm going to tell you about apply specifically to classification models.\n",
    "\n",
    "To create these visualizations, you need to have a model already trained, so that's what I'm doing in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, random_state=42, noise=0.40)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(f\"Total number of samples: {len(X)}\")\n",
    "print(f\"Samples on the test set {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización del dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un clasificador básico para esta lección\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c973445",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is used to evaluate the performance of a classification model. It is a square matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each class in a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96690509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139f548",
   "metadata": {},
   "source": [
    "```plain\n",
    "TP | FP\n",
    "-------\n",
    "FN | TN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fc7a2",
   "metadata": {},
   "source": [
    "The confusion matrix works by comparing the model's predictions with the actual class labels in the test dataset. The main diagonal of the confusion matrix shows the true positives and true negatives, which are the correct predictions of the model. The other entries in the matrix show the false positives and false negatives, which are the incorrect predictions of the model.\n",
    "\n",
    "It is recommended to use it to see what type of predictions your model is failing on, whether in false positives or false negatives. These errors can have important consequences depending on the problem being solved, so it is important to use the confusion matrix to understand and evaluate the model's performance.\n",
    "\n",
    "## Visualization of decision boundaries\n",
    "\n",
    "The `DecisionBoundaryDisplay` function from scikit-learn is a useful tool for visualizing the decision boundaries of a classification model. This function allows us to visualize the regions of the feature space where the model predicts each class, which helps us better understand how the model is making classifications.\n",
    "\n",
    "Visualization of decision boundaries is particularly useful in cases where classes cannot be perfectly separated in the feature space. In these cases, the model may have difficulties making accurate classifications, and the decision boundaries may be irregular or complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    estimator = model, \n",
    "    X = X,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e267253",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "The precision-recall curve is a useful tool for evaluating the performance of a classification model in terms of precision and recall for different classification thresholds. The curve is generated by plotting precision on the *y*-axis and recall on the *x*-axis for different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea028a7b",
   "metadata": {},
   "source": [
    "In other words, it shows the *tradeoff* between precision and recall, where a large area under the curve represents both high recall and high precision. High precision is related to a low false positive rate, and high recall is related to a low false negative rate.\n",
    "\n",
    "This graph is particularly useful when our dataset is imbalanced.\n",
    "\n",
    "## ROC Curve\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a useful visualization for evaluating the performance of a classification model in terms of its ability to distinguish between classes.\n",
    "\n",
    "The curve is generated by plotting the true positive rate (TPR) on the *y*-axis and the false positive rate (FPR) on the *x*-axis for different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c525aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d0990",
   "metadata": {},
   "source": [
    "This allows evaluating the ability of a classification model to distinguish between classes, even when class distributions are unequal. It is useful because it represents the trade-off between the true positive rate and the false positive rate. An ideal classifier is located in the upper-left corner of the graph, indicating a high true positive rate and a low false positive rate. In this case, the model can perfectly distinguish between the two classes.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In summary, visualizations are a powerful tool for understanding and communicating the results of our models more effectively. Scikit-learn offers us several functions to visualize the results of our classification and regression models, and it's important to consider these tools when evaluating and presenting our model results.\n",
    "\n",
    "In the next chapter, we'll discuss what is perhaps the most famous function in scikit-learn. Join me."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
