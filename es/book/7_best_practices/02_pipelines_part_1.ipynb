{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2edc5b4",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1c876",
   "metadata": {},
   "source": [
    "Hello, welcome to this last module in your learning journey about machine learning with scikit-learn. It's time to learn some more tools that will make your models easier to develop and put into production.\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "Pipelines are a sequence of steps to process information.\n",
    "\n",
    "Likewise, following this concept, a pipeline in Scikit-Learn is a way to sequentially apply a list of transformations or predictions to a dataset.\n",
    "\n",
    "Instead of carrying out the execution and storage of each step manually, pipelines allow you to organize pre-processing, feature extraction, and training in one place. And then, you can reuse them when you have to make new predictions.\n",
    "\n",
    "This simplifies your code, provides consistency in your projects, and makes the task of sharing and reusing code very simple.\n",
    "\n",
    "Pipelines follow exactly the same interface that we have already seen shared by many objects in Scikit-Learn.\n",
    "\n",
    "## The `Pipeline` class\n",
    "\n",
    "The class around which everything is centered is the `Pipeline` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd3f01",
   "metadata": {},
   "source": [
    "This receives a list of tuples of transformers associated with a name. For example, let's create a pipeline with two steps: one that scales some variables and another that reduces the dimensions of a dataset – two transformations that we have already seen in this book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b45743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\t('scaler', StandardScaler()),\n",
    "\t('pca', PCA(n_components=2)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61974a60",
   "metadata": {},
   "source": [
    "And now we're going to load some data to demonstrate how it works – note that `X_train` is a matrix with 4 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_split_iris\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_split_iris()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffc56c",
   "metadata": {},
   "source": [
    "With this, we can now train our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ed969",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164a886",
   "metadata": {},
   "source": [
    "After that, we can transform our two datasets – if you look at the resulting values, you'll see that they are now only two dimensions thanks to the dimensionality reduction we added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipeline.transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54826a75",
   "metadata": {},
   "source": [
    "And now, we can use this data in a classifier, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train_transformed, y_train)\n",
    "y_pred = lr.predict(X_test_transformed)\n",
    "score = lr.score(X_test_transformed, y_test)\n",
    "print(f'Test accuracy: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221348b",
   "metadata": {},
   "source": [
    "Excellent, right? Now we don't have to worry about saving the scaler and PCA separately. And now we can use the same pipeline when we put our data into production...\n",
    "\n",
    "## Pipelines as machine learning models\n",
    "\n",
    "But what if I told you that we can include our model as part of the pipeline instead of having it separate?\n",
    "\n",
    "Let's define exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\t('scaler', StandardScaler()),\n",
    "\t('pca', PCA(n_components=2)),\n",
    "\t('lr', LogisticRegression()),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bc7cf",
   "metadata": {},
   "source": [
    "Just as you see it, the last step of a `Pipeline` can be a machine learning model. And then we can use it to predict new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f'Test accuracy: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e710e",
   "metadata": {},
   "source": [
    "## They are compatible with other Scikit-Learn tools\n",
    "\n",
    "`Pipelines` are also compatible with other tools available in Scikit-Learn, for example, the cross-validation tools that we have previously seen in this book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d14e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\t('scaler', StandardScaler()),\n",
    "\t('pca', PCA(n_components=2)),\n",
    "\t('lr', LogisticRegression()),\n",
    "])\n",
    "\n",
    "cv = 5\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f'Scores de validación cruzada ({cv} folds): {cv_scores}')\n",
    "print(f'Score promedio: {np.mean(cv_scores):0.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e124d1",
   "metadata": {},
   "source": [
    "And also with hyperparameter search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Paso 1: Escalar los datos\n",
    "    ('pca', PCA()),               # Paso 2: Reducción de dimensionalidad\n",
    "    ('lr', LogisticRegression()), # Paso 3: Modelo de regresión logística\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [1, 2, 3],\n",
    "    'lr__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'lr__C': np.logspace(-3, 3, 7),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87525082",
   "metadata": {},
   "source": [
    "The peculiarity lies in how we define the parameter grid. You have to use the name with which you associated the transformer followed by two underscores, followed by the name of the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d883894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los resultados\n",
    "print(f'Mejores parámetros: {grid_search.best_params_}')\n",
    "print(f'Mejor puntaje: {grid_search.best_score_:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52b908",
   "metadata": {},
   "source": [
    "Interesting, isn't it?\n",
    "\n",
    "Shall we look a bit more into pipelines and how we can do more complex things with them in the next chapter?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
